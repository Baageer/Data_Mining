### 7.1贝叶斯决策论

​    贝叶斯决策轮(Bayesian decision theory)是概率框架下的实施决策的基本方法。

​    对分类任务来说，在所有相关概率都已知的情况下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。

​    类别标记$У=\{c_1,c_2,...,c_N\}$，$λ_{ij}$是将真实标记为$c_i$的样本误分类为$c_j$所产生的损失，后验概率$P(c_i|x)$。将样本x分类为$c_i$的期望损失(expected loss)，即在样本x上的**条件风险**(conditional risk)为$R(c_i|x)=\sum\limits_{j=1}^Nλ_{ij}P(c_j|x)$。

​    **优化任务**是寻找一个判定标准$h:Х\mapstoУ$以最小化总体风险$R(h)=\mathbb{E}_x[R(h(x)|x)]$。

​    贝叶斯判定准则(Bayes decision rule)：为最小化总体风险，只需在每个样本上选择那个能使条件风险R(c|x)最小的类别标记，即$h^*(x)=\mathop{\arg\min}\limits_{c\inУ}R(c|x)$。

​    $h^*$称为贝叶斯最优分类器(Bayes optimal classifier)，对应的总体风险$R(h^*)$称为贝叶斯风险(Bayes risk)，$1-R(h^*)$反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限。

​    欲使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率$P(c|x)$。然而现实中难以直接获得后验概率。**机器学习所要实现的是基于有限的训练样本尽可能准确的估计出后验概率。**主要有两种策略：

1.判别式模型(discriminative models)，给定x，可直接建模$P(c|x)$来预测c；

2.生成式模型(generative models)，先对联合概率分布$P(x,c)$建模，然后再由此获得$P(c|x)$。

​    决策树、BP神经网络、支持向量机等属于判别式模型。

​    生成式模型需考虑$P(c|x)=\frac{P(x,c)}{P(x)}$，基于贝叶斯定理，$P(c|x)$可写为$P(c|x)=\frac{P(cP(x|c))}{P(x)}$。P(c)是类“先验”(prior)概率；$P(x|c)$是样本x相对于类标记c的类条件概率(class-conditional probability)，或称为似然(likelihood)；$P(x)$是用于归一化的“证据”(evidence)因子。因此估计$P(c|x)$的问题就转化为如何基于训练数据D来估计先验$P(c)$和似然$P(x|c)$。

​    P(c)表达了样本空间中各类样本所占的比例，根据大数定律，当训练集包含**充足的独立同分布样本**时，P(c)可通过各类样本出现的频率来进行估计。

​    P(x|c)可使用极大似然估计预测。直接通过样本频率预计存在1.取值范围大；2.存在没有出现的取值。



### 7.2极大似然估计

​    估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对样本的概率概率分布的参数进行估计。

​    对于参数估计，统计学界的两个学派分别提供了不同的解决方案：

1.**频率主义学派**(Frequentist)认为参数虽然未知，但却是客观存在的固定值，因此可以通过优化似然函数等准则来确定参数值;

2.**贝叶斯学派**(Bayesian)认为参数是未观察到的随机变量，其本身也有分布。可以先假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。

极大似然估计(Maximum Likelihood Estimation,MLE)源自频率主义学派，是根据数据采样来估计概率分布参数的经典方法。

令$D_c$表示训练集D中第c类样本组成的集合，假设这些样本是独立同分布的，则参数$θ_c$对于数据集$D_c$的似然是$P(D_c|θ_c)=\prod\limits_{x\in{D_c}}P(x|θ_c)$。对$θ_c$进行最大似然估计，就是去寻找能最大化似然$P(D_c|θ_c)$的参数值$\hatθ_c$。直观上，极大似然估计是试图在$θ_c$所有可能的取值中，找到一个能使数据出现的“可能性”最大的值。

​    **这种参数化方法虽然能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布。**

### 7.3朴素贝叶斯分类器

​    基于贝叶斯公式来估计后验概率的主要困难在于类条件概率是所有属性上的联合概率，难以从有限的训练样本直接估计而得。为了避开这个障碍，朴素贝叶斯分类器(naive Bayes classifier)采用了属性条件独立性假设(attribute conditional independence assumption)：对一致类别，假设所有属性相互独立。换言之，假设每个属性独立地对分类结果发生影响。

​    基于属性条件独立性假设，$P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod\limits_{i=1}^dP(x_i|c)$，其中d为属性数目，$x_i$为x在第i个属性上的取值。

​    由于对于所有类别来说P(x)相同，因此贝叶斯准则有朴素贝叶斯分类器的表达式为：$h_{nb}=\mathop{\arg\max}\limits_{c\inУ}P(c)\prod\limits_{i=1}^dP(x_i|c)$。



### 7.4半朴素贝叶斯分类器

​    朴素贝叶斯使用的属性独立性假设，在现实任务中往往很难成立。于是对属性条件独立性假设进行一定程度的放松，由此产生一类称为“半朴素贝叶斯分类器”(semi-naive Bayes classifiers)的学习方法。

​    半朴素贝叶斯分类器适当考虑一部分属性间的相互依赖信息，从而既不需进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。

​    “独依赖估计”(One-Dependent Estimator,ODE)是半朴素贝叶斯分类器最常用的一种策略，假设每个属性再类别外最多仅依赖于一个其他属性。

### 7.5贝叶斯网

​    贝叶斯网(Bayesian network)亦称“信念网”(belief network)，借助有向无环图来刻画属性之间的依赖关系，并使用条件概率表(Conditional Probability Table,CPT)来描述属性的联合概率分布。

##### 7.5.1结构

##### 7.5.2学习

##### 7.5.3推断



### 7.6EM算法



