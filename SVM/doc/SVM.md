### 6.1间隔与支持向量机

​    分类问题需要从样本空间中找到一个超平面，将不同的样本分开。能够将样本分开的超平面可能有很多，最合适的应该是位于两类样本正中间的分类超平面。因为这个超平面收样本噪声和样本局限性最小，不容易受到数据扰动的影响，有更强的泛化能力。

​    在样本空间中，划分超平面可以用以下线性方程来描述：

$w^Tx+b=0$，

其中$w=(w_1;w_2;...;2_d)$为法向量，决定超平面的方向；d为位移量，决定了超平面与原点之间的距离。样本空间中任意点x到超平面(w,b)的距离$r=\frac{|w^Tx+b|}{||w||}$。假设超平面(w,b)能够正确对样本进行分类，若$y_i=+1$，则有$w^Tx_i+b>0$；若$y_i=-1$，则有$w^Tx_i+b<0$。令

$\begin{cases}w^Tx_i+b\geq+1, &\mbox{if } y_i=+1 \\  w^Tx_i+b\leq-1, & \mbox{if }y_i=-1  \end{cases}$

距离超平面最近的几个训练样本使得上式成立，称为“支持向量(support vector)”，两个异类支持向量到超平面的距离之和为$γ=\frac{2}{||w||}$，称为“间隔(margin)”。

​    欲找到具有最大间隔的划分超平面，就是找到w和b使得$γ$最大，即

$\max\limits_{w,b}\frac{2}{||w||}$                 $  s.t. y_i(w^Tx_i+b)\geq1,i=1,2,...,m $

可等价于$\min\limits_{w,b} \frac{1}{2}||w||^2$         $  s.t. y_i(w^Tx_i+b)\geq1,i=1,2,...,m $

### 6.2对偶问题

使用拉格朗日乘子法可得到支持向量机基本型的对偶问题(dual problem)，

模型的约束中存在不等式，所以需满足KKT条件。

该对偶问题是是一个二次规划问题，可使用二次规划算法求解，但训练样本很大时，带来很大的计算开销。可改用**SMO算法**。



### 6.3核函数

​    支持向量机的基本型只能解决线性可分的问题，对于非线性可分的问题。如异或问题，在原始的样本空间或许找不到一个划分超平面来划分。对于这样的问题，可将样本映射到一个高维空间，使得样本在这个特征空间内线性可分。

​    另$φ(x)$表示将x映射后的特征向量，则特征空间中划分超面的样本模型为

$f(x)=w^Tφ(x)+b$，

对应SVM参数目标函数为 $\min\limits_{w,b} \frac{1}{2}||w||^2$         $  s.t. y_i(w^Tφ(x_i)+b)\geq1,i=1,2,...,m $

对应对偶问题为$\max\limits_α\sum\limits^m_{i=1}α_i-\frac{1}{2}\sum\limits^m_{i=1}\sum\limits^m_{j=1}α_iα_jy_iy_jφ(x_i)^Tφ(x_j)$

### 6.4软间隔与正则化

​    在理想状况下，可以假定样本空间中的训练样本是线性可分的，但是现实中，很难找到合适的核函数是的训练样本在特征空间中线性可分，即使找到一个线性可分的划分超平面也有可能是过拟合的结果。为了缓解这个问题，需要允许支持向量机再一些样本上出错，所以引入“软间隔”(soft margin)的概念。

$\min\limits_{w,b} \frac{1}{2}||w||^2+C\sum\limits^{m}_{i=1}l_{0/1}(y_i(w^Tx_i+b)-1)$

其中C>0是一个常数，$l_{0/1}$是“0/1损失函数”$l_{0/1}=\begin{cases}1, \mbox{if} z<0 \\ 0, \mbox{otherwise} \end{cases}$



当C为无穷大时，为硬间隔；当C取有限值时，为软间隔。

$l_{0/1}$为非凸，非连续，数学性质不太好，于是使用其他的”替代损失“：

hinge损失：$l_{hinge}=\max(0, 1-z)$；

指数损失：$l_{exp}(z)=exp(-z)$；

对数损失：$l_{log}(z)=log(1+exp(-z))$。





**经验风险（empirical risk）**，用于描述模型与训练数据的契合程度。

**结构风险（structural risk）**，用于描述模型f的某些性质。表述了我们希望获得具有何种性质的模型（例如希望获得复杂度较小的模型），这为引入领域知识和用户意图提供了途径。

$Ω(f)$也被称为**正则项**，C则被称为正则化常数。$L_p$范数是常用的正则化项，其中$L_2$范数$||w||_2$倾向于w的分量取值尽量均衡，即非零分量个数尽量稠密。而$L_0$范数$||w||_0$和$L_1$范数$||w||_0$则倾向于w的分量尽量稀疏，即非零分量个数尽量少，在训练过程中可以起到特征筛选的作用。



### 6.5支持向量回归





### 6.6核方法