### 线性模型
#### 1.基本模型
给定由d个属性描述的示例$\boldsymbol{x}=(x_1;x_2;\ldots{x_d})$，线性模型试图学得一个通过属性的线性组合来进行测的函数
$$f(x)={w_1}{x_1}+{w_2}{x_2}+\ldots{{w_d}{x_d}}+b,$$
向量形式为
$$f(x)={w^T}x+b$$
需要学习的参数就是w和b，通过学习后，模型得以确定。


####2.线性回归
线性回归（linear regression）通过给定训练集，学习一个线性模型，用于预测实值输出标记。
存在一组训练集$(x_i, y_i)$，线性回归试图学得$f(x_i)=w{x_i}+b$使得$f(x_i)\simeq{y_i}$ 即通过学得参数拟合的结果与实际结果相似或者相等。
通过均误方差方程可以衡量f(x)与y之间的差别，让均方误差最小化，即可确定w和b的值。

对于数据集有多个属性，就变成多元线性回归（multivariate linear regression）。同样使用最小二乘法，把w和b写成向量的形式，求得最优解。

·广义线性模型
$$y=g^{-1}({w^T}x+b)$$
函数g(·)为联系函数
当g(·)=ln(·)可将线性模型预测值逼近y的衍生物，如ln(y)，在形式上仍是线性回归，但实际已是在求取输入空间到输出空间的非线性映射函数。

####3.对数几率回归
对于分类任务，可将一个单调可微函数作为联系函数，将分类任务的真实标记y与线性回归模型的预测值联系起来。
对于二分类任务，其输出标记$y\in\lbrace0,1\rbrace$，线性回归模型产生的预测值是实值，可通过“单位跃迁函数”（unit-step function）把实值转化成0/1值。
但是单位跃迁函数不是连续函数，不能作为联系函数，可以使用近似单位跃迁函数的单调可微函数作为替代函数，对数几率函数就是一个常用的替代函数
$$y=\frac{1}{1+e^{-z}}$$
对数几率回归实际上是一种分类学习方法，这个方法的有点有
1.直接对分类可能性进行建模，无需事先假设数据分布；
2.它不是仅预测出“类别”，而是可得到近似概率预测；
3.对率函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数据值优化算法都可直接用于求取最优解。

####4.线性判别分析
线性判别分析（Linear Discriminant Analysis, LDA）的思想：给定训练集，设法投影到一条直线上，使得同类样例尽可能接近、异类样例尽可能远离；对于新样本，将其投影到同一条直线上，根据投影点的位置判断新样本的类别。

####5.多分类学习
多分类学习可以通过二分类方法推广到多分类，主要使用拆分的策略，最经典的拆分策略有三种：一对一，一对其余，多对多

####6.类别不平衡问题
对于类别不平衡学习的一个基本策略就是——再缩放
1.欠采样，去除反例，使得正反例数目接近；
2.过采样，增加正例，使得正反里数目接近；
3.阈值移动，根据正反样本数的比例来调整分类阈值。

